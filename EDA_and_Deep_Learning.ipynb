{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AjjayK/Amex-Credit-Default-Prediction/blob/main/EDA_and_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVYO3JXInHvi"
      },
      "source": [
        "#American Express &ndash; Default Prediction\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:-20.45pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;text-indent:41.75pt;'><span style='font-family:\"Open Sans\",sans-serif;'>Predict if a customer will default in the future</span></p>\n",
        "\n",
        "---\n",
        "\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:-20.45pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;text-indent:41.75pt;'><span style='font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:-20.45pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;text-indent:41.75pt;'><strong><span style='font-size:19px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>About American Express</span></strong></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>American Express Company (Amex) is an American multinational corporation specialized in payment card services headquartered at 200 Vesey Street in the Battery Park City neighborhood of Lower Manhattan in New York City. The company was founded in 1850 and is one of the 30 components of the Dow Jones Industrial Average. The company&apos;s logo, adopted in 1958, is a gladiator or centurion whose image appears on the company&apos;s well-known traveler&apos;s cheques, charge cards, and credit cards.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>During the 1980s, Amex invested in the brokerage industry, acquiring what became, in increments, Shearson Lehman Hutton and then divesting these into what became Smith Barney Shearson (owned by Primerica) and a revived Lehman Brothers. By 2008 neither the Shearson nor the Lehman name existed.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>In 2016, credit cards using the American Express network accounted for 22.9% of the total dollar volume of credit card transactions in the United States. As of December 31, 2021, the company had 121.7 million cards in force, including 56.4 million cards in force in the United States, each with an average annual spending of $20,392.</span></p>\n",
        "\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:0cm;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>In 2017, Forbes named American Express as the 23rd most valuable brand in the world (and the highest within financial services), estimating the brand to be worth US$24.5 billion. In 2020, Fortune magazine ranked American Express at number 9 on their Fortune List of the Top 100 Companies to Work For in 2020 based on an employee survey of satisfaction</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "\n",
        "---\n",
        "\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><strong><span style='font-size:19px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>About Competition</span></strong></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>American Express hosted this competition in Kaggle. Quoted below is the description of the competition provided in the Kaggle site.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>Whether out at a restaurant or buying tickets to a concert, modern life counts on the convenience of a credit card to make daily purchases. It saves us from carrying large amounts of cash and also can advance a full purchase that can be paid overtime. How do card issuers know we&rsquo;ll pay back what we charge? That&rsquo;s a complex problem with many existing solutions&mdash;and even more potential improvements, to be explored in this competition.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>Credit default prediction is central to managing risk in a consumer lending business. Credit default prediction allows lenders to optimize lending decisions, which leads to a better customer experience and sound business economics. Current models exist to help manage risk. But it&apos;s possible to create better models that can outperform those currently in use.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>American Express is a globally integrated payments company. The largest payment card issuer in the world, they provide customers with access to products, insights, and experiences that enrich lives and build business success.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>In this competition, you&rsquo;ll apply your machine learning skills to predict credit default. Specifically, you will leverage an industrial scale data set to build a machine learning model that challenges the current model in production. Training, validation, and testing datasets include time-series behavioral data and anonymized customer profile information. You&apos;re free to explore any technique to create the most powerful model, from creating features to using the data in a more organic way within a model.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>If successful, you&apos;ll help create a better customer experience for cardholders by making it easier to be approved for a credit card. Top solutions could challenge the credit default prediction model used by the world&apos;s largest payment card issuer&mdash;earning you cash prizes, the opportunity to interview with American Express, and potentially a rewarding new career.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><strong><span style='font-size:19px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>About Dataset</span></strong></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>The objective of this competition is to predict the probability that a customer does not pay back their credit card balance amount in the future based on their monthly customer profile. The target binary variable is calculated by observing 18 months performance window after the latest credit card statement, and if the customer does not pay due amount in 120 days after their latest statement date it is considered a default event.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>The dataset contains aggregated profile features for each customer at each statement date. Features are anonymized and normalized, and fall into the following general categories:</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>D_* = Delinquency variables</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>S_* = Spend variables</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>P_* = Payment variables</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>B_* = Balance variables</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>R_* = Risk variables</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>with the following features being categorical:</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>[&apos;B_30&apos;, &apos;B_38&apos;, &apos;D_114&apos;, &apos;D_116&apos;, &apos;D_117&apos;, &apos;D_120&apos;, &apos;D_126&apos;, &apos;D_63&apos;, &apos;D_64&apos;, &apos;D_66&apos;, &apos;D_68&apos;]</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>The task is to predict, for each customer_ID, the probability of a future payment default (target = 1).</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>Note that the negative class has been subsampled for this dataset at 5%, and thus receives a 20x weighting in the scoring metric</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "\n",
        "---\n",
        "\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><strong><span style='font-size:19px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>About Notebook</span></strong></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>This notebook consists of Exploratory Data Analysis (EDA) tool for analyzing the variable characteristics and Deep Neural Decision Tree, Deep Neural Decision Forest models for predicting the credit default.</span></p>"
      ],
      "id": "yVYO3JXInHvi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYy72QkcsJzw"
      },
      "source": [
        "###**Initializing the notebook**\n",
        "The notebook runs on Google Colab. Google Colab uses Python 3.7."
      ],
      "id": "oYy72QkcsJzw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sGNz_uyyvwZ",
        "outputId": "305ea6da-d89d-4621-ba37-37d00f1db50c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "#Mouting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "id": "6sGNz_uyyvwZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70ec2fe6"
      },
      "outputs": [],
      "source": [
        "#Importing the libraries used in this notebook\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "'''#Data Viz\n",
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "from dash import Dash, dcc, html, Input, Output\n",
        "from plotly.subplots import make_subplots\n",
        "import dash_bootstrap_components as dbc'''\n",
        "\n",
        "#Tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import math\n",
        "\n",
        "'''#LGBM\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from lightgbm import LGBMClassifier, early_stopping, log_evaluation'''\n",
        "\n",
        "#System\n",
        "import warnings, gc\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "70ec2fe6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4LkLAvB0U3k"
      },
      "source": [
        "###**Loading the Dataset**\n",
        "Dataset was downloaded from Kaggle and is copied to the drive"
      ],
      "id": "D4LkLAvB0U3k"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLl2BY0eUiHx",
        "outputId": "7133c278-5e9e-4dff-d776-0757f6c4d76a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (6.0.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyarrow"
      ],
      "id": "lLl2BY0eUiHx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de5b380e"
      },
      "outputs": [],
      "source": [
        "#Loading the training data\n",
        "train_data = pd.read_feather(\"/content/drive/MyDrive/Amex Kaggle/train_data.ftr\")\n",
        "train_data = train_data.groupby('customer_ID').tail(1).set_index('customer_ID')"
      ],
      "id": "de5b380e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBYQH-3H095Y"
      },
      "source": [
        "###**EDA Tool**"
      ],
      "id": "fBYQH-3H095Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72348341"
      },
      "outputs": [],
      "source": [
        "#Column stratification\n",
        "col = train_data.columns.to_list()\n",
        "cat_features = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "non_cat_features = [x for x in col if x not in cat_features]\n",
        "\n",
        "delinquency_variables = [i for i in col if i.startswith('D_')]\n",
        "spend_variables = [i for i in col if (i.startswith('S_') and i != 'S_2')]\n",
        "payment_variables = [i for i in col if i.startswith('P_')]\n",
        "balance_variables = [i for i in col if i.startswith('B_')]\n",
        "risk_variables = [i for i in col if i.startswith('R_')]"
      ],
      "id": "72348341"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "ea96937f",
        "outputId": "72dd1db1-a27f-4e0a-c376-5ec54465abbe"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Creating a html site using Dash Library for EDA\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m external_stylesheets \u001b[38;5;241m=\u001b[39m [\u001b[43mdbc\u001b[49m\u001b[38;5;241m.\u001b[39mthemes\u001b[38;5;241m.\u001b[39mSOLAR]\n\u001b[1;32m      3\u001b[0m app \u001b[38;5;241m=\u001b[39m Dash(\u001b[38;5;18m__name__\u001b[39m, external_stylesheets\u001b[38;5;241m=\u001b[39mexternal_stylesheets)\n\u001b[1;32m      4\u001b[0m all_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelinquency_variables\u001b[39m\u001b[38;5;124m'\u001b[39m:delinquency_variables,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspend_variables\u001b[39m\u001b[38;5;124m'\u001b[39m:spend_variables,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayment_variables\u001b[39m\u001b[38;5;124m'\u001b[39m:payment_variables,\n\u001b[1;32m      5\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalance_variables\u001b[39m\u001b[38;5;124m'\u001b[39m:balance_variables,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrisk_variables\u001b[39m\u001b[38;5;124m'\u001b[39m:risk_variables}\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dbc' is not defined"
          ]
        }
      ],
      "source": [
        "#Creating a html site using Dash Library for EDA\n",
        "external_stylesheets = [dbc.themes.SOLAR]\n",
        "app = Dash(__name__, external_stylesheets=external_stylesheets)\n",
        "all_options = {'delinquency_variables':delinquency_variables,'spend_variables':spend_variables,'payment_variables':payment_variables,\n",
        "              'balance_variables':balance_variables,'risk_variables':risk_variables}\n",
        "\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H3(\"Variable Information Dashboard\"),\n",
        "    html.Div([\n",
        "    html.H6(\"Select the variable type\"),\n",
        "    dcc.Dropdown(\n",
        "    list(all_options.keys()),\n",
        "    'delinquency_variables',\n",
        "    id = 'variable-type'),\n",
        "    html.H6(\"Select the variable\"),\n",
        "    dcc.Dropdown(id = 'variable')]),\n",
        "    \n",
        "    dcc.Graph(id = 'dist-plot'),\n",
        "    dcc.Graph(id = 'heatmaps')\n",
        "])\n",
        "\n",
        "@app.callback(\n",
        "Output('variable','options'),\n",
        "Input('variable-type','value'))\n",
        "def set_variable_options(variable_type):\n",
        "    return [{'label': i, 'value': i} for i in all_options[variable_type]]\n",
        "\n",
        "@app.callback(\n",
        "Output('variable','value'),\n",
        "    Input('variable','options'))\n",
        "def set_variable_value(available_options):\n",
        "    return available_options[0]['value']\n",
        "\n",
        "@app.callback(\n",
        "Output('dist-plot','figure'),\n",
        "Output('heatmaps','figure'),\n",
        "Input('variable-type','value'),\n",
        "Input('variable','value'))\n",
        "def hist_plot(variable_type, variable):\n",
        "    fig1 = make_subplots(rows=1, cols=2,column_widths=[0.7, 0.3],\n",
        "    subplot_titles=(\"Histogram for \"+ variable_type +\" \"+variable,\"Boxplot for \"+ variable_type +\" \"+variable))\n",
        "\n",
        "    traces_nd = go.Histogram(name= \"Non-Defaulting\", x = train_data[variable][train_data['target']==0].to_list(),histnorm='percent',nbinsx = 50, marker_color = 'indianred' )\n",
        "    traces_d = go.Histogram(name = \"Defaulting\", x  = train_data[variable][train_data['target']==1].to_list(),histnorm='percent',nbinsx = 50, marker_color = 'lightseagreen' )\n",
        "    fig1.append_trace(traces_nd,1,1)\n",
        "    fig1.append_trace(traces_d,1,1)\n",
        "\n",
        "    trace_box_nd = go.Box(y = train_data[variable][train_data['target']==0].to_list(), name = \"Non-Defaulting\", marker_color = 'indianred')\n",
        "    trace_box_d = go.Box(y = train_data[variable][train_data['target']==1].to_list(), name = \"Defaulting\", marker_color = 'lightseagreen')\n",
        "\n",
        "\n",
        "    fig1.append_trace(trace_box_nd,1,2)\n",
        "    fig1.append_trace(trace_box_d,1,2)\n",
        "\n",
        "    fig1.update_layout(barmode='overlay')\n",
        "    fig1.update_traces(opacity=0.75)\n",
        "    \n",
        "    cor = train_data[all_options[variable_type]].corr()\n",
        "    cor = cor[variable].sort_values(ascending = False)\n",
        "\n",
        "    fig2 = go.Figure()\n",
        "    fig2.add_trace(go.Bar(x = cor[cor>=0], y = cor.index[cor>=0], orientation = 'h', marker_color='rgb(167, 255, 235)', marker_line_color='rgb(0, 105, 92)',\n",
        "                      marker_line_width=1.5, opacity=0.6, showlegend = False, text = cor[cor>=0], textposition = 'outside', texttemplate = '%{x:.3f}' ))\n",
        "    fig2.add_trace(go.Bar(x=cor[cor<0].sort_values(ascending = False), y = cor[cor<0].sort_values(ascending = False).index,orientation = 'h',marker_color='rgb(255, 138, 128)', marker_line_color='rgb(198, 40, 40)',\n",
        "                      marker_line_width=1.5, opacity=0.6, showlegend = False, text = cor[cor<0].sort_values(ascending = False), textposition = 'outside',texttemplate = '%{x:.3f}'))\n",
        "    fig2.update_layout(title=\"Correlation Plot for \" + variable_type +\" \"+variable,\n",
        "                      xaxis_title=\"Correlation\", margin=dict(l=150),\n",
        "                      height=2500)\n",
        "\n",
        "    \n",
        "    return fig1, fig2"
      ],
      "id": "ea96937f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69241b89",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#Firing the Dash app\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=False)\n",
        "\n",
        "\n",
        "#Click on the hyperlink to see the visualization"
      ],
      "id": "69241b89"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "horAzC2r3tKQ"
      },
      "source": [
        "###**Preparing the dataset for deep learning and machine learning models**"
      ],
      "id": "horAzC2r3tKQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c55cc193"
      },
      "outputs": [],
      "source": [
        "#Dropping the customer ID column\n",
        "train_data.drop(columns = ['S_2'], inplace = True)\n",
        "train_data.to_csv('train_data_csv.csv', index = False, header = False)\n",
        "\n",
        "\n",
        "test_data = pd.read_feather(\"/content/drive/MyDrive/Amex Kaggle/test_data.ftr\")\n",
        "test_data = test_df.groupby('customer_ID').tail(1).set_index('customer_ID')\n",
        "test_data['target'] = np.nan\n",
        "test_data.drop(columns = ['S_2'], inplace = True)\n",
        "test_data.to_csv('test_data_csv.csv', index = False, header = False)"
      ],
      "id": "c55cc193"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upPWXxjS5O88"
      },
      "source": [
        "###**Classification with Neural Decision Tree and Forest**"
      ],
      "id": "upPWXxjS5O88"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3fa97aa"
      },
      "outputs": [],
      "source": [
        "#Segregating categorical and numerical features\n",
        "CSV_HEADER = train_data.columns.to_list()\n",
        "cat_features = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "NUMERIC_FEATURE_NAMES = [x for x in CSV_HEADER if x not in cat_features]\n",
        "NUMERIC_FEATURE_NAMES.remove('target')"
      ],
      "id": "d3fa97aa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdc64a3c"
      },
      "outputs": [],
      "source": [
        "# A list of the numerical feature names.\n",
        "\n",
        "# A dictionary of the categorical features and their vocabulary.\n",
        "\n",
        "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
        "    \"B_30\": sorted(list(train_data[\"B_30\"].unique())),\n",
        "    \"B_38\": sorted(list(train_data[\"B_38\"].unique())),\n",
        "    \"D_114\": sorted(list(train_data[\"D_114\"].unique())),\n",
        "    \"D_116\": sorted(list(train_data[\"D_116\"].unique())),\n",
        "    \"D_117\": sorted(list(train_data[\"D_117\"].unique())),\n",
        "    \"D_120\": sorted(list(train_data[\"D_120\"].unique())),\n",
        "    \"D_126\": sorted(list(train_data[\"D_126\"].unique())),\n",
        "    \"D_63\": sorted(list(train_data[\"D_63\"].unique())),\n",
        "    \"D_64\": sorted(list(train_data[\"D_64\"].unique())),\n",
        "    \"D_66\": sorted(list(train_data[\"D_66\"].unique())),\n",
        "    \"D_68\": sorted(list(train_data[\"D_68\"].unique()))\n",
        "}\n",
        "# A list of the columns to ignore from the dataset.\n",
        "IGNORE_COLUMN_NAMES = [\"customer_ID\"]\n",
        "# A list of the categorical feature names.\n",
        "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
        "# A list of all the input features.\n",
        "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
        "# A list of column default values for each feature.\n",
        "COLUMN_DEFAULTS = [\n",
        "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + IGNORE_COLUMN_NAMES else [\"NA\"]\n",
        "    for feature_name in CSV_HEADER\n",
        "]\n",
        "# The name of the target feature.\n",
        "TARGET_FEATURE_NAME = \"target\"\n",
        "# A list of the labels of the target features.\n",
        "TARGET_LABELS = [\"0\", \"1\"]"
      ],
      "id": "bdc64a3c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjjqJTqh6VzO"
      },
      "source": [
        "####**Create tf.data.Dataset objects for training and validation**\n",
        "We create an input function to read and parse the file, and convert features and labels into a [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for training and validation. We also preprocess the input by mapping the target label to an index."
      ],
      "id": "tjjqJTqh6VzO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a05c63a"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import StringLookup\n",
        "\n",
        "target_label_lookup = StringLookup(\n",
        "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
        ")\n",
        "target_label_lookup2 = StringLookup(\n",
        "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=1\n",
        ")\n",
        "\n",
        "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_defaults=COLUMN_DEFAULTS,\n",
        "        label_name=TARGET_FEATURE_NAME,\n",
        "        num_epochs=1,\n",
        "        na_value=\"?\",\n",
        "        shuffle=shuffle,\n",
        "        header = False,\n",
        "        column_names = CSV_HEADER\n",
        "    ).map(lambda features, target: (features, target_label_lookup(target)))\n",
        "    return dataset.cache()\n",
        "\n",
        "def get_dataset_from_csv2(csv_file_path, shuffle=False, batch_size=128):\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_defaults=COLUMN_DEFAULTS,\n",
        "        label_name=TARGET_FEATURE_NAME,\n",
        "        num_epochs=1,\n",
        "        na_value=\"?\",\n",
        "        shuffle=shuffle,\n",
        "        header = False,\n",
        "        column_names = CSV_HEADER\n",
        "    ).map(lambda features, target: (features, target_label_lookup2(target)))\n",
        "    return dataset.cache()"
      ],
      "id": "2a05c63a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klfXXic16nSr"
      },
      "source": [
        "####**Create model inputs**"
      ],
      "id": "klfXXic16nSr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "392b2ef5"
      },
      "outputs": [],
      "source": [
        "def create_model_inputs():\n",
        "    inputs = {}\n",
        "    for feature_name in FEATURE_NAMES:\n",
        "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
        "            inputs[feature_name] = layers.Input(\n",
        "                name=feature_name, shape=(), dtype=tf.float32\n",
        "            )\n",
        "        else:\n",
        "            inputs[feature_name] = layers.Input(\n",
        "                name=feature_name, shape=(), dtype=tf.string\n",
        "            )\n",
        "    return inputs"
      ],
      "id": "392b2ef5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4n5rzH46p1y"
      },
      "source": [
        "####**Encode input features**"
      ],
      "id": "D4n5rzH46p1y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a972bdd"
      },
      "outputs": [],
      "source": [
        "def encode_inputs(inputs):\n",
        "    encoded_features = []\n",
        "    for feature_name in inputs:\n",
        "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
        "            vocabulary = [str(x) for x in CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]]\n",
        "            # Create a lookup to convert a string values to an integer indices.\n",
        "            # Since we are not using a mask token, nor expecting any out of vocabulary\n",
        "            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
        "            lookup = StringLookup(\n",
        "                vocabulary=vocabulary, mask_token=None, num_oov_indices=1\n",
        "            )\n",
        "            # Convert the string input values into integer indices.\n",
        "            value_index = lookup(inputs[feature_name])\n",
        "            embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n",
        "            # Create an embedding layer with the specified dimensions.\n",
        "            embedding = layers.Embedding(\n",
        "                input_dim=lookup.vocabulary_size(), output_dim=embedding_dims\n",
        "            )\n",
        "            # Convert the index values to embedding representations.\n",
        "            encoded_feature = embedding(value_index)\n",
        "        else:\n",
        "            # Use the numerical features as-is.\n",
        "            encoded_feature = inputs[feature_name]\n",
        "            if inputs[feature_name].shape[-1] is None:\n",
        "                encoded_feature = tf.expand_dims(encoded_feature, -1)\n",
        "\n",
        "        encoded_features.append(encoded_feature)\n",
        "\n",
        "    encoded_features = layers.concatenate(encoded_features)\n",
        "    return encoded_features"
      ],
      "id": "5a972bdd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPWCvTfb6sg6"
      },
      "source": [
        "####**Deep Neural Decision Tree**\n",
        "A neural decision tree model has two sets of weights to learn. The first set is pi, which represents the probability distribution of the classes in the tree leaves. The second set is the weights of the routing layer decision_fn, which represents the probability of going to each leave. The forward pass of the model works as follows:\n",
        "\n",
        "\n",
        "\n",
        "1.   The model expects input features as a single vector encoding all the features of an instance in the batch. This vector can be generated from a Convolution Neural Network (CNN) applied to images or dense transformations applied to structured data features.\n",
        "2.   The model first applies a used_features_mask to randomly select a subset of input features to use.\n",
        "3.   Then, the model computes the probabilities (mu) for the input instances to reach the tree leaves by iteratively performing a stochastic routing throughout the tree levels.\n",
        "4.   Finally, the probabilities of reaching the leaves are combined by the class probabilities at the leaves to produce the final outputs."
      ],
      "id": "xPWCvTfb6sg6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5348f9ef"
      },
      "outputs": [],
      "source": [
        "class NeuralDecisionTree(keras.Model):\n",
        "    def __init__(self, depth, num_features, used_features_rate, num_classes):\n",
        "        super(NeuralDecisionTree, self).__init__()\n",
        "        self.depth = depth\n",
        "        self.num_leaves = 2 ** depth\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Create a mask for the randomly selected features.\n",
        "        num_used_features = int(num_features * used_features_rate)\n",
        "        one_hot = np.eye(num_features)\n",
        "        sampled_feature_indicies = np.random.choice(\n",
        "            np.arange(num_features), num_used_features, replace=False\n",
        "        )\n",
        "        self.used_features_mask = one_hot[sampled_feature_indicies]\n",
        "\n",
        "        # Initialize the weights of the classes in leaves.\n",
        "        self.pi = tf.Variable(\n",
        "            initial_value=tf.random_normal_initializer()(\n",
        "                shape=[self.num_leaves, self.num_classes]\n",
        "            ),\n",
        "            dtype=\"float32\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        # Initialize the stochastic routing layer.\n",
        "        self.decision_fn = layers.Dense(\n",
        "            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n",
        "        )\n",
        "\n",
        "    def call(self, features):\n",
        "        batch_size = tf.shape(features)[0]\n",
        "\n",
        "        # Apply the feature mask to the input features.\n",
        "        features = tf.matmul(\n",
        "            features, self.used_features_mask, transpose_b=True\n",
        "        )  # [batch_size, num_used_features]\n",
        "        # Compute the routing probabilities.\n",
        "        decisions = tf.expand_dims(\n",
        "            self.decision_fn(features), axis=2\n",
        "        )  # [batch_size, num_leaves, 1]\n",
        "        # Concatenate the routing probabilities with their complements.\n",
        "        decisions = layers.concatenate(\n",
        "            [decisions, 1 - decisions], axis=2\n",
        "        )  # [batch_size, num_leaves, 2]\n",
        "\n",
        "        mu = tf.ones([batch_size, 1, 1])\n",
        "\n",
        "        begin_idx = 1\n",
        "        end_idx = 2\n",
        "        # Traverse the tree in breadth-first order.\n",
        "        for level in range(self.depth):\n",
        "            mu = tf.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]\n",
        "            mu = tf.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n",
        "            level_decisions = decisions[\n",
        "                :, begin_idx:end_idx, :\n",
        "            ]  # [batch_size, 2 ** level, 2]\n",
        "            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n",
        "            begin_idx = end_idx\n",
        "            end_idx = begin_idx + 2 ** (level + 1)\n",
        "\n",
        "        mu = tf.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]\n",
        "        probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]\n",
        "        outputs = tf.matmul(mu, probabilities)  # [batch_size, num_classes]\n",
        "        return outputs"
      ],
      "id": "5348f9ef"
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "5f2c177a"
      },
      "outputs": [],
      "source": [
        "#Fitting the Neural Decision Tree\n",
        "learning_rate = 0.01\n",
        "batch_size = 1000\n",
        "num_epochs = 5\n",
        "hidden_units = [64, 64]\n",
        "\n",
        "\n",
        "def run_experiment(model):\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    train_dataset = get_dataset_from_csv(\n",
        "        'train_data_csv.csv', shuffle=False, batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    model.fit(train_dataset, epochs=num_epochs)\n",
        "    print(\"Model training finished\")\n",
        "\n",
        "    return model"
      ],
      "id": "5f2c177a"
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "SsOTH2qYLcPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e449a845-64a0-4db0-a3c1-6880179fc6bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training the model...\n",
            "Epoch 1/5\n",
            "459/459 [==============================] - 73s 150ms/step - loss: 0.2909 - sparse_categorical_accuracy: 0.8938\n",
            "Epoch 2/5\n",
            "459/459 [==============================] - 10s 22ms/step - loss: 0.2325 - sparse_categorical_accuracy: 0.8988\n",
            "Epoch 3/5\n",
            "459/459 [==============================] - 10s 21ms/step - loss: 0.2271 - sparse_categorical_accuracy: 0.9004\n",
            "Epoch 4/5\n",
            "459/459 [==============================] - 10s 21ms/step - loss: 0.2237 - sparse_categorical_accuracy: 0.9021\n",
            "Epoch 5/5\n",
            "459/459 [==============================] - 10s 21ms/step - loss: 0.2211 - sparse_categorical_accuracy: 0.9038\n",
            "Model training finished\n"
          ]
        }
      ],
      "source": [
        "#Creating the model\n",
        "num_trees = 10\n",
        "depth = 8\n",
        "used_features_rate = 1.0\n",
        "num_classes = len(TARGET_LABELS)\n",
        "\n",
        "\n",
        "def create_tree_model():\n",
        "    inputs = create_model_inputs()\n",
        "    features = encode_inputs(inputs)\n",
        "    features = layers.BatchNormalization()(features)\n",
        "    num_features = features.shape[1]\n",
        "\n",
        "    tree = NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
        "\n",
        "    outputs = tree(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "tree_model = create_tree_model()\n",
        "decision_tree = run_experiment(tree_model)"
      ],
      "id": "SsOTH2qYLcPw"
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "-g_76ilJK9-M"
      },
      "outputs": [],
      "source": [
        "#Predicting the response using model for training data and test data\n",
        "decision_tree_ypred_train = decision_tree.predict(get_dataset_from_csv(\n",
        "        'train_data_csv.csv', shuffle=False, batch_size=batch_size\n",
        "    ))\n",
        "\n",
        "decision_tree_ypred_test = decision_tree.predict(get_dataset_from_csv2(\n",
        "        'test_data_csv.csv', shuffle=False, batch_size=batch_size\n",
        "    ))\n"
      ],
      "id": "-g_76ilJK9-M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfekfuxP7XGH"
      },
      "source": [
        "####**Deep Neural Decision Forest**\n",
        "The neural decision forest model consists of a set of neural decision trees that are trained simultaneously. The output of the forest model is the average outputs of its trees."
      ],
      "id": "BfekfuxP7XGH"
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "0e1c323f"
      },
      "outputs": [],
      "source": [
        "class NeuralDecisionForest(keras.Model):\n",
        "    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n",
        "        super(NeuralDecisionForest, self).__init__()\n",
        "        self.ensemble = []\n",
        "        # Initialize the ensemble by adding NeuralDecisionTree instances.\n",
        "        # Each tree will have its own randomly selected input features to use.\n",
        "        for _ in range(num_trees):\n",
        "            self.ensemble.append(\n",
        "                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
        "            )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        outputs = tf.zeros([batch_size, num_classes])\n",
        "\n",
        "        # Aggregate the outputs of trees in the ensemble.\n",
        "        for tree in self.ensemble:\n",
        "            outputs += tree(inputs)\n",
        "        # Divide the outputs by the ensemble size to get the average.\n",
        "        outputs /= len(self.ensemble)\n",
        "        return outputs"
      ],
      "id": "0e1c323f"
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "066bc2b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "605dc3a1-e0db-4d1d-c71b-ff06d08a3789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training the model...\n",
            "Epoch 1/5\n",
            "459/459 [==============================] - 87s 156ms/step - loss: 0.2867 - sparse_categorical_accuracy: 0.8953\n",
            "Epoch 2/5\n",
            "459/459 [==============================] - 23s 49ms/step - loss: 0.2283 - sparse_categorical_accuracy: 0.9003\n",
            "Epoch 3/5\n",
            "459/459 [==============================] - 23s 49ms/step - loss: 0.2228 - sparse_categorical_accuracy: 0.9018\n",
            "Epoch 4/5\n",
            "459/459 [==============================] - 23s 49ms/step - loss: 0.2190 - sparse_categorical_accuracy: 0.9033\n",
            "Epoch 5/5\n",
            "459/459 [==============================] - 22s 49ms/step - loss: 0.2159 - sparse_categorical_accuracy: 0.9043\n",
            "Model training finished\n"
          ]
        }
      ],
      "source": [
        "#Fitting the forest model\n",
        "num_trees = 25\n",
        "depth = 5\n",
        "used_features_rate = 1\n",
        "num_classes = len(TARGET_LABELS)\n",
        "\n",
        "def create_forest_model():\n",
        "    inputs = create_model_inputs()\n",
        "    features = encode_inputs(inputs)\n",
        "    features = layers.BatchNormalization()(features)\n",
        "    num_features = features.shape[1]\n",
        "\n",
        "    forest_model = NeuralDecisionForest(\n",
        "        num_trees, depth, num_features, used_features_rate, num_classes\n",
        "    )\n",
        "\n",
        "    outputs = forest_model(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "forest_model = create_forest_model()\n",
        "\n",
        "fitted_forest = run_experiment(forest_model)"
      ],
      "id": "066bc2b1"
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "70b7f6cf"
      },
      "outputs": [],
      "source": [
        "#Predicting the response using model for training data and test data\n",
        "forest_y_pred_train = fitted_forest.predict(get_dataset_from_csv(\n",
        "        'train_data_csv.csv', shuffle=False, batch_size=batch_size\n",
        "    ))\n",
        "forest_y_pred_test = fitted_forest.predict(get_dataset_from_csv2(\n",
        "        'test_data_csv.csv', shuffle=False, batch_size=batch_size\n",
        "    ))\n"
      ],
      "id": "70b7f6cf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Model Evaluation**\n",
        "#####**Amex Metrics**\n",
        "The evaluation metric, M, for this competition is the mean of two measures of rank ordering: Normalized Gini Coefficient, G, and default rate captured at 4%, D.\n",
        "\n",
        "M=0.5⋅(G+D)\n",
        "The default rate captured at 4% is the percentage of the positive labels (defaults) captured within the highest-ranked 4% of the predictions, and represents a Sensitivity/Recall statistic.\n",
        "\n",
        "For both of the sub-metrics G and D, the negative labels are given a weight of 20 to adjust for downsampling.\n",
        "\n",
        "This metric has a maximum value of 1.0.\n",
        "#####**Sparse Categorical Accuracy**\n",
        "Since interpretation of Amex metrics is difficult, sparse categorical accuracy is estimated for the training data. As 'target' values are not avilable for test data, we only evaluate this metric for training data. It was found that higher the categorical accruacy on training data, higher the Amex metrics on test data. Hence, high value of the categorical accuracy on training data should give a better estimate for higher categorical accuracy for test data."
      ],
      "metadata": {
        "id": "qFjbWh_Ocbzj"
      },
      "id": "qFjbWh_Ocbzj"
    },
    {
      "cell_type": "code",
      "source": [
        "#Sparse Categorical Accuracy\n",
        "def sparse_categorical_accuracy(y_true, y_pred):\n",
        "  acc = np.dot(1, np.equal(y_true, np.argmax(y_pred, axis=1)))\n",
        "  return (sum(acc)/len(acc))*100"
      ],
      "metadata": {
        "id": "wBxW__uwVGbI"
      },
      "id": "wBxW__uwVGbI",
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Amex Metrics\n",
        "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
        "\n",
        "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
        "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
        "              .sort_values('prediction', ascending=False))\n",
        "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
        "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
        "        df['weight_cumsum'] = df['weight'].cumsum()\n",
        "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
        "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
        "        \n",
        "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
        "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
        "              .sort_values('prediction', ascending=False))\n",
        "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
        "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
        "        total_pos = (df['target'] * df['weight']).sum()\n",
        "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
        "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
        "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
        "        return df['gini'].sum()\n",
        "\n",
        "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
        "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
        "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
        "\n",
        "    g = normalized_weighted_gini(y_true, y_pred)\n",
        "    d = top_four_percent_captured(y_true, y_pred)\n",
        "\n",
        "    return 0.5 * (g + d)"
      ],
      "metadata": {
        "id": "jJCnU1-VWA7S"
      },
      "id": "jJCnU1-VWA7S",
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the sparse categorical accuracy for training data\n",
        "y_true = train_data['target']\n",
        "print(\"Sparse categorical accuracy for deep decision tree model: {:.5f}%\".format(sparse_categorical_accuracy(y_true,decision_tree_ypred_train)))\n",
        "print(\"Sparse categorical accuracy for deep forest model: {:.5f}%\".format(sparse_categorical_accuracy(y_true,forest_y_pred_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBx14frVWJxV",
        "outputId": "2a3c3d2e-2bc3-4527-e4bf-462b07f37ad5"
      },
      "id": "vBx14frVWJxV",
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparse categorical accuracy for deep decision tree model: 90.45527%\n",
            "Sparse categorical accuracy for deep forest model: 90.58667%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the amex metrics for training data\n",
        "print(\"Amex metric for deep decision tree model: {:.5f}\".format(amex_metric(y_true.to_frame().reset_index(),pd.DataFrame(np.argmax(decision_tree_ypred_train, axis=1), columns = ['prediction']))))\n",
        "print(\"Amex metric for deep forest model: {:.5f}\".format(amex_metric(y_true.to_frame().reset_index(),pd.DataFrame(np.argmax(forest_y_pred_train, axis=1), columns = ['prediction']))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb3ARPsBXBW7",
        "outputId": "655e85a2-d562-40aa-fc4d-6fc932586af3"
      },
      "id": "Hb3ARPsBXBW7",
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amex metric for deep decision tree model: 0.59020\n",
            "Amex metric for deep forest model: 0.59137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the amex metrics for training data\n",
        "print(\"Amex metric for deep decision tree model: {:.5f}\".format(amex_metric(y_true.to_frame(),pd.DataFrame(decision_tree_ypred_train[:,1], columns = ['prediction'], index = train_data.index))))\n",
        "print(\"Amex metric for deep forest model: {:.5f}\".format(amex_metric(y_true.to_frame(),pd.DataFrame(forest_y_pred_train[:,1], columns = ['prediction'],index = train_data.index))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a8df07-a918-451a-a64e-ef3bf4338813",
        "id": "Z3Fiej_Mo2yO"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amex metric for deep decision tree model: 0.79070\n",
            "Amex metric for deep forest model: 0.80014\n"
          ]
        }
      ],
      "id": "Z3Fiej_Mo2yO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test Data Evaluation\n",
        "Using Kaggle API to submit to the competition. Enter your kaggle username and kaggle key"
      ],
      "metadata": {
        "id": "w0a2JAL0gla9"
      },
      "id": "w0a2JAL0gla9"
    },
    {
      "cell_type": "code",
      "source": [
        "decision_tree_sub = pd.DataFrame(decision_tree_ypred_test[:,1], columns = ['prediction'], index =test_df.index)\n",
        "decision_tree_sub.to_csv('decision_tree_sub.csv')\n"
      ],
      "metadata": {
        "id": "Y2Q4ci5IgivR"
      },
      "id": "Y2Q4ci5IgivR",
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_sub = pd.DataFrame(forest_y_pred_test[:,1], columns = ['prediction'], index =test_df.index)\n",
        "forest_sub.to_csv('forest.csv')"
      ],
      "metadata": {
        "id": "Lm7em2GqsDNw"
      },
      "id": "Lm7em2GqsDNw",
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_username'] = username\n",
        "os.environ['KAGGLE_key'] = key\n",
        "import kaggle"
      ],
      "metadata": {
        "id": "BW6qrC8Vi2l0"
      },
      "id": "BW6qrC8Vi2l0",
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c amex-default-prediction -f /content/decision_tree_sub.csv -m decision_tree_sub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1VJQVJ0ivkc",
        "outputId": "c49d71a2-00cd-4900-adc8-019708296eb0"
      },
      "id": "b1VJQVJ0ivkc",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 67.6M/67.6M [00:06<00:00, 11.7MB/s]\n",
            "Successfully submitted to American Express - Default Prediction"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c amex-default-prediction -f /content/forest.csv -m forest_sub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pe-EOvz1tF9D",
        "outputId": "debd90cc-fafd-4f7f-c57e-34efff0d8204"
      },
      "id": "pe-EOvz1tF9D",
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 67.6M/67.6M [00:06<00:00, 11.6MB/s]\n",
            "Successfully submitted to American Express - Default Prediction"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submissions -c amex-default-prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zD_pARNtypl",
        "outputId": "58294bc6-d3f7-4b61-ffac-d9130fc8f128"
      },
      "id": "0zD_pARNtypl",
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fileName               date                 description                status    publicScore  privateScore  \n",
            "---------------------  -------------------  -------------------------  --------  -----------  ------------  \n",
            "forest.csv             2022-09-10 18:42:36  forest_sub                 complete  0.78227      0.79177       \n",
            "decision_tree_sub.csv  2022-09-10 18:42:27  decision_tree_sub          complete  0.77162      0.78181       \n",
            "lgbm_out.csv           2022-09-10 18:41:38  lgbm                       complete  0.78604      0.79546       \n",
            "lgbm_out.csv           2022-09-10 18:20:40  Submitting again for lgbm  complete  0.78604      0.79546       \n",
            "lgbm_out.csv           2022-09-10 18:17:53  lgbm baseline              complete  0.57899      0.56586       \n",
            "decision_tree_sub.csv  2022-09-10 18:01:25  decision_tree_sub          complete  0.56312      0.56595       \n",
            "submission2 (7).csv    2022-08-27 17:58:53                             complete  0.77662      0.78733       \n",
            "submission2 (6).csv    2022-08-27 16:15:12                             complete  0.77609      0.78574       \n",
            "submission2 (5).csv    2022-08-27 15:51:28                             complete  0.01980      0.01715       \n",
            "submission2 (4).csv    2022-08-27 15:42:35                             error                                \n",
            "submission2 (3).csv    2022-08-14 18:12:47                             complete  0.73622      0.74040       \n",
            "submission2 (2).csv    2022-08-13 21:29:32                             complete  0.74105      0.74414       \n",
            "submission2 (1).csv    2022-08-13 04:35:10                             complete  0.72229      0.72738       \n",
            "submission2.csv        2022-08-12 18:53:53                             complete  0.73238      0.73784       \n",
            "submission (1).csv     2022-08-12 18:29:41  Neural Decision Forests    complete  0.47605      0.47018       \n",
            "submission.csv         2022-08-12 18:08:19  Neural Decision Forests    complete  -0.44719     -0.44667      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Conclusion**\n",
        "\n",
        "1.   LGBM (Model in other notebook) stands out to be better in terms of metrics (Score - 0.79546 and Categorical Accuracy on training data - 92.87%)\n",
        "2.   Amex Metric for Deep Decision Forest Model was better than Deep Neural Decision Tree since Forest Model is an ensemble model\n",
        "\n",
        "Overall results are given in the table below\n",
        "\n",
        "<table style=\"margin-left:21.3pt;border-collapse:collapse;border:none;\">\n",
        "    <tbody>\n",
        "        <tr>\n",
        "            <td style=\"width: 134.85pt;border: 1pt solid windowtext;padding: 0cm 5.4pt;vertical-align: top;\">\n",
        "                <p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:0cm;line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;'><strong><span style='font-size:16px;font-family:\"Open Sans\",sans-serif;'>Model</span></strong></p>\n",
        "            </td>\n",
        "            <td style=\"width: 134.85pt;border-top: 1pt solid windowtext;border-right: 1pt solid windowtext;border-bottom: 1pt solid windowtext;border-image: initial;border-left: none;padding: 0cm 5.4pt;vertical-align: top;\">\n",
        "                <p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:0cm;line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;'><strong><span style='font-size:16px;font-family:\"Open Sans\",sans-serif;'>Categorical Accuracy on Training Data</span></strong></p>\n",
        "            </td>\n",
        "            <td style=\"width: 134.9pt;border-top: 1pt solid windowtext;border-right: 1pt solid windowtext;border-bottom: 1pt solid windowtext;border-image: initial;border-left: none;padding: 0cm 5.4pt;vertical-align: top;\">\n",
        "                <p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:0cm;line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;'><strong><span style='font-size:16px;font-family:\"Open Sans\",sans-serif;'>Amex Metric on Training Data</span></strong></p>\n",
        "            </td>\n",
        "            <td style=\"width: 134.9pt;border-top: 1pt solid windowtext;border-right: 1pt solid windowtext;border-bottom: 1pt solid windowtext;border-image: initial;border-left: none;padding: 0cm 5.4pt;vertical-align: top;\">\n",
        "                <p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:0cm;line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;'><strong><span style='font-size:16px;font-family:\"Open Sans\",sans-serif;'>Amex Metric on Test Data</span></strong></p>\n",
        "            </td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td style=\"width: 134.85pt;border-right: 1pt solid windowtext;border-bottom: 1pt solid windowtext;border-left: 1pt solid windowtext;border-image: initial;border-top: none;padding: 0cm 5.4pt;vertical-align: top;\">\n",
        "                <p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:0cm;line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;font-family:\"Open Sans\",sans-serif;'>LGBM</span></p>\n",
        "            </td>\n",
        "            <td style=\"width: 134.85pt;border-top: none;border-left: none;border-bottom: 1pt solid windowtext;border-right: 1pt solid windowtext;padding: 0cm 5.4pt;vertical-align: top;\">\n",
        "                <p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:0cm;line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;text-align:center;'><span style='font-size:16px;font-family:\"Open Sans\",sans-serif;'>92.8759%</span></p>\n",
        "            </td>\n",
        "            <td style=\"width: 134.9pt;border-top: none;border-left: none;border-bottom: 1pt solid windowtext;border-right: 1pt solid windowtext;padding: 0cm 5.4pt;vertical-align: top;\">\n",
        "                <p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:0cm;line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;text-align:center;'><span style='font-size:16px;font-family:\"Open Sans\",sans-serif;'>0.6813</span></p>\n",
        "            </td>\n",
        "            <td style=\"width: 134.9pt;border-top: none;border-left: none;border-bottom: 1pt solid windowtext;border-right: 1pt solid windowtext;padding: 0cm 5.4pt;vertical-align: top;\">\n",
        "                <p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:0cm;line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;text-align:center;'><span style='font-size:16px;font-family:\"Open Sans\",sans-serif;'>0.7954</span></p>\n",
        "            </td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td style=\"width: 134.85pt;border-right: 1pt solid windowtext;border-bottom: 1pt solid windowtext;border-left: 1pt solid windowtext;border-image: initial;border-top: none;padding: 0cm 5.4pt;vertical-align: top;\">\n",
        "                <p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:0cm;line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;font-family:\"Open Sans\",sans-serif;'>Deep Neural Decision Tree</span></p>\n",
        "            </td>\n",
        "            <td style=\"width: 134.85pt;border-top: none;border-left: none;border-bottom: 1pt solid windowtext;border-right: 1pt solid windowtext;padding: 0cm 5.4pt;vertical-align: top;\">\n",
        "                <p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:0cm;line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;text-align:center;'><span style='font-size:16px;font-family:\"Open Sans\",sans-serif;'>90.4552%</span></p>\n",
        "            </td>\n",
        "            <td style=\"width: 134.9pt;border-top: none;border-left: none;border-bottom: 1pt solid windowtext;border-right: 1pt solid windowtext;padding: 0cm 5.4pt;vertical-align: top;\">\n",
        "                <p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:0cm;line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;text-align:center;'><span style='font-size:16px;font-family:\"Open Sans\",sans-serif;'>0.7907</span></p>\n",
        "            </td>\n",
        "            <td style=\"width: 134.9pt;border-top: none;border-left: none;border-bottom: 1pt solid windowtext;border-right: 1pt solid windowtext;padding: 0cm 5.4pt;vertical-align: top;\">\n",
        "                <p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:0cm;line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;text-align:center;'><span style='font-size:16px;font-family:\"Open Sans\",sans-serif;'>0.7818</span></p>\n",
        "            </td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td style=\"width: 134.85pt;border-right: 1pt solid windowtext;border-bottom: 1pt solid windowtext;border-left: 1pt solid windowtext;border-image: initial;border-top: none;padding: 0cm 5.4pt;vertical-align: top;\">\n",
        "                <p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:0cm;line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;font-family:\"Open Sans\",sans-serif;'>Deep Neural Decision Forest</span></p>\n",
        "            </td>\n",
        "            <td style=\"width: 134.85pt;border-top: none;border-left: none;border-bottom: 1pt solid windowtext;border-right: 1pt solid windowtext;padding: 0cm 5.4pt;vertical-align: top;\">\n",
        "                <p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:0cm;line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;text-align:center;'><span style='font-size:16px;font-family:\"Open Sans\",sans-serif;'>90.5866%</span></p>\n",
        "            </td>\n",
        "            <td style=\"width: 134.9pt;border-top: none;border-left: none;border-bottom: 1pt solid windowtext;border-right: 1pt solid windowtext;padding: 0cm 5.4pt;vertical-align: top;\">\n",
        "                <p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:0cm;line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;text-align:center;'><span style='font-size:16px;font-family:\"Open Sans\",sans-serif;'>0.8001</span></p>\n",
        "            </td>\n",
        "            <td style=\"width: 134.9pt;border-top: none;border-left: none;border-bottom: 1pt solid windowtext;border-right: 1pt solid windowtext;padding: 0cm 5.4pt;vertical-align: top;\">\n",
        "                <p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:0cm;line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;text-align:center;'><span style='font-size:16px;font-family:\"Open Sans\",sans-serif;'>0.7917</span></p>\n",
        "            </td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "</table>"
      ],
      "metadata": {
        "id": "fDhxeX-Gt-tq"
      },
      "id": "fDhxeX-Gt-tq"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}