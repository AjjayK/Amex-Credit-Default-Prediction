{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AjjayK/Amex-Credit-Default-Prediction/blob/main/DefaultPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:-20.45pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;text-indent:41.75pt;'><strong><span style='font-size:30pt;line-height:107%;font-family:\"Open Sans\",sans-serif;'>American Express &ndash; Default Prediction</span></strong></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:-20.45pt;line-height:107%;font-size:30px;font-family:\"Calibri\",sans-serif;text-indent:41.75pt;'><span style='font-family:\"Open Sans\",sans-serif;'>Predict if a customer will default in the future</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:-20.45pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;text-indent:41.75pt;'><span style='font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:-20.45pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;text-indent:41.75pt;'><strong><span style='font-size:19px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>About American Express</span></strong></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>American Express Company (Amex) is an American multinational corporation specialized in payment card services headquartered at 200 Vesey Street in the Battery Park City neighborhood of Lower Manhattan in New York City. The company was founded in 1850 and is one of the 30 components of the Dow Jones Industrial Average. The company&apos;s logo, adopted in 1958, is a gladiator or centurion whose image appears on the company&apos;s well-known traveler&apos;s cheques, charge cards, and credit cards.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>During the 1980s, Amex invested in the brokerage industry, acquiring what became, in increments, Shearson Lehman Hutton and then divesting these into what became Smith Barney Shearson (owned by Primerica) and a revived Lehman Brothers. By 2008 neither the Shearson nor the Lehman name existed.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>In 2016, credit cards using the American Express network accounted for 22.9% of the total dollar volume of credit card transactions in the United States. As of December 31, 2021, the company had 121.7 million cards in force, including 56.4 million cards in force in the United States, each with an average annual spending of $20,392.</span></p>\n",
        "\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:0cm;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>In 2017, Forbes named American Express as the 23rd most valuable brand in the world (and the highest within financial services), estimating the brand to be worth US$24.5 billion. In 2020, Fortune magazine ranked American Express at number 9 on their Fortune List of the Top 100 Companies to Work For in 2020 based on an employee survey of satisfaction</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><strong><span style='font-size:19px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>About Competition</span></strong></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>American Express hosted this competition in Kaggle. Quoted below is the description of the competition provided in the Kaggle site.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>Whether out at a restaurant or buying tickets to a concert, modern life counts on the convenience of a credit card to make daily purchases. It saves us from carrying large amounts of cash and also can advance a full purchase that can be paid overtime. How do card issuers know we&rsquo;ll pay back what we charge? That&rsquo;s a complex problem with many existing solutions&mdash;and even more potential improvements, to be explored in this competition.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>Credit default prediction is central to managing risk in a consumer lending business. Credit default prediction allows lenders to optimize lending decisions, which leads to a better customer experience and sound business economics. Current models exist to help manage risk. But it&apos;s possible to create better models that can outperform those currently in use.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>American Express is a globally integrated payments company. The largest payment card issuer in the world, they provide customers with access to products, insights, and experiences that enrich lives and build business success.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>In this competition, you&rsquo;ll apply your machine learning skills to predict credit default. Specifically, you will leverage an industrial scale data set to build a machine learning model that challenges the current model in production. Training, validation, and testing datasets include time-series behavioral data and anonymized customer profile information. You&apos;re free to explore any technique to create the most powerful model, from creating features to using the data in a more organic way within a model.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>If successful, you&apos;ll help create a better customer experience for cardholders by making it easier to be approved for a credit card. Top solutions could challenge the credit default prediction model used by the world&apos;s largest payment card issuer&mdash;earning you cash prizes, the opportunity to interview with American Express, and potentially a rewarding new career.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><strong><span style='font-size:19px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>About Dataset</span></strong></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>The objective of this competition is to predict the probability that a customer does not pay back their credit card balance amount in the future based on their monthly customer profile. The target binary variable is calculated by observing 18 months performance window after the latest credit card statement, and if the customer does not pay due amount in 120 days after their latest statement date it is considered a default event.</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>The dataset contains aggregated profile features for each customer at each statement date. Features are anonymized and normalized, and fall into the following general categories:</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>D_* = Delinquency variables</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>S_* = Spend variables</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>P_* = Payment variables</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>B_* = Balance variables</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>R_* = Risk variables</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>with the following features being categorical:</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>[&apos;B_30&apos;, &apos;B_38&apos;, &apos;D_114&apos;, &apos;D_116&apos;, &apos;D_117&apos;, &apos;D_120&apos;, &apos;D_126&apos;, &apos;D_63&apos;, &apos;D_64&apos;, &apos;D_66&apos;, &apos;D_68&apos;]</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>The task is to predict, for each customer_ID, the probability of a future payment default (target = 1).</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>Note that the negative class has been subsampled for this dataset at 5%, and thus receives a 20x weighting in the scoring metric</span></p>\n",
        "<p style='margin-top:0cm;margin-right:0cm;margin-bottom:8.0pt;margin-left:21.3pt;line-height:107%;font-size:15px;font-family:\"Calibri\",sans-serif;'><span style='font-size:16px;line-height:107%;font-family:\"Open Sans\",sans-serif;'>&nbsp;</span></p>"
      ],
      "metadata": {
        "id": "yVYO3JXInHvi"
      },
      "id": "yVYO3JXInHvi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**About Notebook**\n"
      ],
      "metadata": {
        "id": "oYy72QkcsJzw"
      },
      "id": "oYy72QkcsJzw"
    },
    {
      "cell_type": "code",
      "source": [
        "#Upgrading to Python 3.9\n",
        "#Ignore if you have notebook running on Python 3.9 \n",
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py39_4.9.2-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter\n",
        "!conda install -q -y google-colab -c conda-forge\n",
        "!python -m ipykernel install --name \"py39\" --user"
      ],
      "metadata": {
        "id": "uL1B6exvP2ZX"
      },
      "id": "uL1B6exvP2ZX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dash\n",
        "!pip install dash_bootstrap_components\n",
        "!pip install lightgbm\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G2Cw2bUQMHo4",
        "outputId": "c3c3a723-4419-4518-aa46-8301f86dab65"
      },
      "id": "G2Cw2bUQMHo4",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dash in /usr/local/lib/python3.9/site-packages (2.6.1)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.9/site-packages (from dash) (2.0.0)\n",
            "Requirement already satisfied: Flask>=1.0.4 in /usr/local/lib/python3.9/site-packages (from dash) (2.2.2)\n",
            "Requirement already satisfied: flask-compress in /usr/local/lib/python3.9/site-packages (from dash) (1.12)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.9/site-packages (from dash) (5.10.0)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.9/site-packages (from dash) (2.0.0)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.9/site-packages (from dash) (5.0.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.9/site-packages (from Flask>=1.0.4->dash) (2.2.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.9/site-packages (from Flask>=1.0.4->dash) (2.1.2)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.9/site-packages (from Flask>=1.0.4->dash) (3.0.3)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.9/site-packages (from Flask>=1.0.4->dash) (4.12.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.9/site-packages (from Flask>=1.0.4->dash) (8.1.3)\n",
            "Requirement already satisfied: Flask>=1.0.4 in /usr/local/lib/python3.9/site-packages (from dash) (2.2.2)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.9/site-packages (from flask-compress->dash) (1.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=3.6.0->Flask>=1.0.4->dash) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from Jinja2>=3.0->Flask>=1.0.4->dash) (2.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/site-packages (from plotly>=5.0.0->dash) (8.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from Jinja2>=3.0->Flask>=1.0.4->dash) (2.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dash_bootstrap_components in /usr/local/lib/python3.9/site-packages (1.2.1)\n",
            "Requirement already satisfied: dash>=2.0.0 in /usr/local/lib/python3.9/site-packages (from dash_bootstrap_components) (2.6.1)\n",
            "Requirement already satisfied: Flask>=1.0.4 in /usr/local/lib/python3.9/site-packages (from dash>=2.0.0->dash_bootstrap_components) (2.2.2)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.9/site-packages (from dash>=2.0.0->dash_bootstrap_components) (2.0.0)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.9/site-packages (from dash>=2.0.0->dash_bootstrap_components) (5.0.0)\n",
            "Requirement already satisfied: flask-compress in /usr/local/lib/python3.9/site-packages (from dash>=2.0.0->dash_bootstrap_components) (1.12)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.9/site-packages (from dash>=2.0.0->dash_bootstrap_components) (2.0.0)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.9/site-packages (from dash>=2.0.0->dash_bootstrap_components) (5.10.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.9/site-packages (from Flask>=1.0.4->dash>=2.0.0->dash_bootstrap_components) (8.1.3)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.9/site-packages (from Flask>=1.0.4->dash>=2.0.0->dash_bootstrap_components) (2.2.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.9/site-packages (from Flask>=1.0.4->dash>=2.0.0->dash_bootstrap_components) (2.1.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.9/site-packages (from Flask>=1.0.4->dash>=2.0.0->dash_bootstrap_components) (4.12.0)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.9/site-packages (from Flask>=1.0.4->dash>=2.0.0->dash_bootstrap_components) (3.0.3)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.9/site-packages (from flask-compress->dash>=2.0.0->dash_bootstrap_components) (1.0.9)\n",
            "Requirement already satisfied: Flask>=1.0.4 in /usr/local/lib/python3.9/site-packages (from dash>=2.0.0->dash_bootstrap_components) (2.2.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=3.6.0->Flask>=1.0.4->dash>=2.0.0->dash_bootstrap_components) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from Jinja2>=3.0->Flask>=1.0.4->dash>=2.0.0->dash_bootstrap_components) (2.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/site-packages (from plotly>=5.0.0->dash>=2.0.0->dash_bootstrap_components) (8.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from Jinja2>=3.0->Flask>=1.0.4->dash>=2.0.0->dash_bootstrap_components) (2.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.9/site-packages (3.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from lightgbm) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0 in /usr/local/lib/python3.9/site-packages (from lightgbm) (1.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/site-packages (from lightgbm) (1.9.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/site-packages (from lightgbm) (0.36.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/site-packages (from lightgbm) (1.9.1)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from lightgbm) (1.21.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from lightgbm) (1.21.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 578.1 MB 24 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.21.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from tensorflow) (51.0.0.post20201207)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/site-packages (from tensorflow) (4.3.0)\n",
            "Collecting absl-py>=1.0.0\n",
            "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 89.8 MB/s \n",
            "\u001b[?25hCollecting astunparse>=1.6.0\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.1)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting google-pasta>=0.1.1\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Downloading grpcio-1.48.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 63.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
            "Collecting h5py>=2.9.0\n",
            "  Downloading h5py-3.7.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 76.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.21.5)\n",
            "Collecting keras<2.11,>=2.10.0\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 67.0 MB/s \n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.21.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
            "Collecting libclang>=13.0.0\n",
            "  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.1 MB 63.0 MB/s \n",
            "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.21.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.9)\n",
            "Collecting protobuf<3.20,>=3.9.2\n",
            "  Downloading protobuf-3.19.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 79.3 MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.11,>=2.10\n",
            "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 65.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.25.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.11.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.21.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from tensorflow) (51.0.0.post20201207)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.7)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.11.0)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2022.6.15.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.25.11)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.25.0)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.1-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 86.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 55.4 MB/s \n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[K     |████████████████████████████████| 781 kB 76.9 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.11,>=2.10.0\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 105.7 MB/s \n",
            "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.27.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 89.3 MB/s \n",
            "\u001b[?25hCollecting termcolor>=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
            "Collecting wrapt>=1.11.0\n",
            "  Downloading wrapt-1.14.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: termcolor\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=c73d0939ce6be0d6f112b77a0d8a64c70263dee92f96526fcb685424e2f5dec7\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
            "Successfully built termcolor\n",
            "Installing collected packages: oauthlib, requests-oauthlib, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
            "Successfully installed absl-py-1.2.0 astunparse-1.6.3 flatbuffers-2.0.7 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.48.1 h5py-3.7.0 keras-2.10.0 keras-preprocessing-1.1.2 libclang-14.0.6 markdown-3.4.1 oauthlib-3.2.1 opt-einsum-3.3.0 protobuf-3.19.4 requests-oauthlib-1.3.1 tensorboard-2.10.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.27.0 termcolor-1.1.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2"
      ],
      "metadata": {
        "id": "vBHkQ8FGQqdY"
      },
      "id": "vBHkQ8FGQqdY",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "70ec2fe6",
      "metadata": {
        "id": "70ec2fe6"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import math\n",
        "import gc\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import math\n",
        "import zipfile\n",
        "\n",
        "import os\n",
        "import plotly\n",
        "from dash import Dash, dcc, html, Input, Output\n",
        "import numpy as np\n",
        "import dash_bootstrap_components as dbc\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
        "import warnings, gc\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de5b380e",
      "metadata": {
        "id": "de5b380e"
      },
      "outputs": [],
      "source": [
        "#Loading the training data\n",
        "train_data = pd.read_feather(r\"C:\\Users\\ajjay\\Amex Kaggle\\train_data.ftr\")\n",
        "train_data = train_data.groupby('customer_ID').tail(1).set_index('customer_ID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72348341",
      "metadata": {
        "id": "72348341"
      },
      "outputs": [],
      "source": [
        "#Column stratification\n",
        "col = train_data.columns.to_list()\n",
        "cat_features = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "non_cat_features = [x for x in col if x not in cat_features]\n",
        "\n",
        "delinquency_variables = [i for i in col if i.startswith('D_')]\n",
        "spend_variables = [i for i in col if (i.startswith('S_') and i != 'S_2')]\n",
        "payment_variables = [i for i in col if i.startswith('P_')]\n",
        "balance_variables = [i for i in col if i.startswith('B_')]\n",
        "risk_variables = [i for i in col if i.startswith('R_')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea96937f",
      "metadata": {
        "id": "ea96937f"
      },
      "outputs": [],
      "source": [
        "external_stylesheets = [dbc.themes.SOLAR]\n",
        "app = Dash(__name__, external_stylesheets=external_stylesheets)\n",
        "all_options = {'delinquency_variables':delinquency_variables,'spend_variables':spend_variables,'payment_variables':payment_variables,\n",
        "              'balance_variables':balance_variables,'risk_variables':risk_variables}\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H3(\"Variable Information Dashboard\"),\n",
        "    html.Div([\n",
        "    html.H6(\"Select the variable type\"),\n",
        "    dcc.Dropdown(\n",
        "    list(all_options.keys()),\n",
        "    'delinquency_variables',\n",
        "    id = 'variable-type'),\n",
        "    html.H6(\"Select the variable\"),\n",
        "    dcc.Dropdown(id = 'variable')]),\n",
        "    \n",
        "    dcc.Graph(id = 'dist-plot'),\n",
        "    dcc.Graph(id = 'heatmaps')\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "@app.callback(\n",
        "Output('variable','options'),\n",
        "Input('variable-type','value'))\n",
        "def set_variable_options(variable_type):\n",
        "    return [{'label': i, 'value': i} for i in all_options[variable_type]]\n",
        "\n",
        "@app.callback(\n",
        "Output('variable','value'),\n",
        "    Input('variable','options'))\n",
        "def set_variable_value(available_options):\n",
        "    return available_options[0]['value']\n",
        "\n",
        "@app.callback(\n",
        "Output('dist-plot','figure'),\n",
        "Output('heatmaps','figure'),\n",
        "Input('variable-type','value'),\n",
        "Input('variable','value'))\n",
        "def hist_plot(variable_type, variable):\n",
        "    fig1 = make_subplots(rows=1, cols=2,column_widths=[0.7, 0.3],\n",
        "    subplot_titles=(\"Histogram for \"+ variable_type +\" \"+variable,\"Boxplot for \"+ variable_type +\" \"+variable))\n",
        "\n",
        "    traces_nd = go.Histogram(name= \"Non-Defaulting\", x = train_data[variable][train_data['target']==0].to_list(),histnorm='percent',nbinsx = 50, marker_color = 'indianred' )\n",
        "    traces_d = go.Histogram(name = \"Defaulting\", x  = train_data[variable][train_data['target']==1].to_list(),histnorm='percent',nbinsx = 50, marker_color = 'lightseagreen' )\n",
        "    fig1.append_trace(traces_nd,1,1)\n",
        "    fig1.append_trace(traces_d,1,1)\n",
        "\n",
        "    trace_box_nd = go.Box(y = train_data[variable][train_data['target']==0].to_list(), name = \"Non-Defaulting\", marker_color = 'indianred')\n",
        "    trace_box_d = go.Box(y = train_data[variable][train_data['target']==1].to_list(), name = \"Defaulting\", marker_color = 'lightseagreen')\n",
        "\n",
        "\n",
        "    fig1.append_trace(trace_box_nd,1,2)\n",
        "    fig1.append_trace(trace_box_d,1,2)\n",
        "\n",
        "    fig1.update_layout(barmode='overlay')\n",
        "    fig1.update_traces(opacity=0.75)\n",
        "    \n",
        "    cor = train_data[all_options[variable_type]].corr()\n",
        "    cor = cor[variable].sort_values(ascending = False)\n",
        "\n",
        "    fig2 = go.Figure()\n",
        "    fig2.add_trace(go.Bar(x = cor[cor>=0], y = cor.index[cor>=0], orientation = 'h', marker_color='rgb(167, 255, 235)', marker_line_color='rgb(0, 105, 92)',\n",
        "                      marker_line_width=1.5, opacity=0.6, showlegend = False, text = cor[cor>=0], textposition = 'outside', texttemplate = '%{x:.3f}' ))\n",
        "    fig2.add_trace(go.Bar(x=cor[cor<0].sort_values(ascending = False), y = cor[cor<0].sort_values(ascending = False).index,orientation = 'h',marker_color='rgb(255, 138, 128)', marker_line_color='rgb(198, 40, 40)',\n",
        "                      marker_line_width=1.5, opacity=0.6, showlegend = False, text = cor[cor<0].sort_values(ascending = False), textposition = 'outside',texttemplate = '%{x:.3f}'))\n",
        "    fig2.update_layout(title=\"Correlation Plot for \" + variable_type +\" \"+variable,\n",
        "                      xaxis_title=\"Correlation\", margin=dict(l=150),\n",
        "                      height=2500)\n",
        "\n",
        "    \n",
        "    return fig1, fig2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69241b89",
      "metadata": {
        "scrolled": true,
        "id": "69241b89",
        "outputId": "b844c42f-0890-4061-b32d-5e2381841465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dash is running on http://127.0.0.1:8050/\n",
            "\n",
            "Dash is running on http://127.0.0.1:8050/\n",
            "\n",
            "Dash is running on http://127.0.0.1:8050/\n",
            "\n",
            "Dash is running on http://127.0.0.1:8050/\n",
            "\n",
            "Dash is running on http://127.0.0.1:8050/\n",
            "\n",
            "Dash is running on http://127.0.0.1:8050/\n",
            "\n",
            "Dash is running on http://127.0.0.1:8050/\n",
            "\n",
            "Dash is running on http://127.0.0.1:8050/\n",
            "\n",
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "   WARNING: This is a development server. Do not use it in a production deployment.\n",
            "   Use a production WSGI server instead.\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
            "127.0.0.1 - - [06/Sep/2022 20:19:31] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [06/Sep/2022 20:19:31] \"\u001b[37mGET /_dash-layout HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [06/Sep/2022 20:19:31] \"\u001b[37mGET /_dash-dependencies HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [06/Sep/2022 20:19:31] \"\u001b[37mGET /_dash-component-suites/dash/dcc/async-graph.js HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [06/Sep/2022 20:19:31] \"\u001b[37mGET /_dash-component-suites/dash/dcc/async-dropdown.js HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [06/Sep/2022 20:19:31] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [06/Sep/2022 20:19:31] \"\u001b[37mGET /_dash-component-suites/dash/dcc/async-plotlyjs.js HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [06/Sep/2022 20:19:31] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [06/Sep/2022 20:19:41] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [06/Sep/2022 20:19:58] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [06/Sep/2022 20:20:28] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [06/Sep/2022 20:21:18] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c55cc193",
      "metadata": {
        "id": "c55cc193"
      },
      "outputs": [],
      "source": [
        "train_data.drop(columns = ['S_2'], inplace = True)\n",
        "df.to_csv('train_data_csv.csv', index = False, header = False)\n",
        "\n",
        "\n",
        "test_df = pd.read_feather(r\"C:\\Users\\ajjay\\Amex Kaggle\\test_data.ftr\")\n",
        "test_df = test_df.groupby('customer_ID').tail(1).set_index('customer_ID')\n",
        "test_df.drop(columns = ['S_2'], inplace = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3fa97aa",
      "metadata": {
        "id": "d3fa97aa"
      },
      "outputs": [],
      "source": [
        "CSV_HEADER = df.columns.to_list()\n",
        "cat_features = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "NUMERIC_FEATURE_NAMES = [x for x in CSV_HEADER if x not in cat_features]\n",
        "NUMERIC_FEATURE_NAMES.remove('target')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdc64a3c",
      "metadata": {
        "id": "bdc64a3c"
      },
      "outputs": [],
      "source": [
        "# A list of the numerical feature names.\n",
        "\n",
        "# A dictionary of the categorical features and their vocabulary.\n",
        "\n",
        "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
        "    \"B_30\": sorted(list(df[\"B_30\"].unique())),\n",
        "    \"B_38\": sorted(list(df[\"B_38\"].unique())),\n",
        "    \"D_114\": sorted(list(df[\"D_114\"].unique())),\n",
        "    \"D_116\": sorted(list(df[\"D_116\"].unique())),\n",
        "    \"D_117\": sorted(list(df[\"D_117\"].unique())),\n",
        "    \"D_120\": sorted(list(df[\"D_120\"].unique())),\n",
        "    \"D_126\": sorted(list(df[\"D_126\"].unique())),\n",
        "    \"D_63\": sorted(list(df[\"D_63\"].unique())),\n",
        "    \"D_64\": sorted(list(df[\"D_64\"].unique())),\n",
        "    \"D_66\": sorted(list(df[\"D_66\"].unique())),\n",
        "    \"D_68\": sorted(list(df[\"D_68\"].unique()))\n",
        "}\n",
        "# A list of the columns to ignore from the dataset.\n",
        "IGNORE_COLUMN_NAMES = [\"customer_ID\"]\n",
        "# A list of the categorical feature names.\n",
        "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
        "# A list of all the input features.\n",
        "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
        "# A list of column default values for each feature.\n",
        "COLUMN_DEFAULTS = [\n",
        "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + IGNORE_COLUMN_NAMES else [\"NA\"]\n",
        "    for feature_name in CSV_HEADER\n",
        "]\n",
        "# The name of the target feature.\n",
        "TARGET_FEATURE_NAME = \"target\"\n",
        "# A list of the labels of the target features.\n",
        "TARGET_LABELS = [\"0\", \"1\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a05c63a",
      "metadata": {
        "id": "2a05c63a"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import StringLookup\n",
        "\n",
        "target_label_lookup = StringLookup(\n",
        "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
        ")\n",
        "target_label_lookup2 = StringLookup(\n",
        "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=1\n",
        ")\n",
        "\n",
        "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_defaults=COLUMN_DEFAULTS,\n",
        "        label_name=TARGET_FEATURE_NAME,\n",
        "        num_epochs=1,\n",
        "        na_value=\"?\",\n",
        "        shuffle=shuffle,\n",
        "        header = False,\n",
        "        column_names = CSV_HEADER\n",
        "    ).map(lambda features, target: (features, target_label_lookup(target)))\n",
        "    return dataset.cache()\n",
        "\n",
        "def get_dataset_from_csv2(csv_file_path, shuffle=False, batch_size=128):\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_defaults=COLUMN_DEFAULTS,\n",
        "        label_name=TARGET_FEATURE_NAME,\n",
        "        num_epochs=1,\n",
        "        na_value=\"?\",\n",
        "        shuffle=shuffle,\n",
        "        header = False,\n",
        "        column_names = CSV_HEADER\n",
        "    ).map(lambda features, target: (features, target_label_lookup2(target)))\n",
        "    return dataset.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "392b2ef5",
      "metadata": {
        "id": "392b2ef5"
      },
      "outputs": [],
      "source": [
        "def create_model_inputs():\n",
        "    inputs = {}\n",
        "    for feature_name in FEATURE_NAMES:\n",
        "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
        "            inputs[feature_name] = layers.Input(\n",
        "                name=feature_name, shape=(), dtype=tf.float32\n",
        "            )\n",
        "        else:\n",
        "            inputs[feature_name] = layers.Input(\n",
        "                name=feature_name, shape=(), dtype=tf.string\n",
        "            )\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a972bdd",
      "metadata": {
        "id": "5a972bdd"
      },
      "outputs": [],
      "source": [
        "def encode_inputs(inputs):\n",
        "    encoded_features = []\n",
        "    for feature_name in inputs:\n",
        "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
        "            vocabulary = [str(x) for x in CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]]\n",
        "            # Create a lookup to convert a string values to an integer indices.\n",
        "            # Since we are not using a mask token, nor expecting any out of vocabulary\n",
        "            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
        "            lookup = StringLookup(\n",
        "                vocabulary=vocabulary, mask_token=None, num_oov_indices=1\n",
        "            )\n",
        "            # Convert the string input values into integer indices.\n",
        "            value_index = lookup(inputs[feature_name])\n",
        "            embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n",
        "            # Create an embedding layer with the specified dimensions.\n",
        "            embedding = layers.Embedding(\n",
        "                input_dim=lookup.vocabulary_size(), output_dim=embedding_dims\n",
        "            )\n",
        "            # Convert the index values to embedding representations.\n",
        "            encoded_feature = embedding(value_index)\n",
        "        else:\n",
        "            # Use the numerical features as-is.\n",
        "            encoded_feature = inputs[feature_name]\n",
        "            if inputs[feature_name].shape[-1] is None:\n",
        "                encoded_feature = tf.expand_dims(encoded_feature, -1)\n",
        "\n",
        "        encoded_features.append(encoded_feature)\n",
        "\n",
        "    encoded_features = layers.concatenate(encoded_features)\n",
        "    return encoded_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5348f9ef",
      "metadata": {
        "id": "5348f9ef"
      },
      "outputs": [],
      "source": [
        "class NeuralDecisionTree(keras.Model):\n",
        "    def __init__(self, depth, num_features, used_features_rate, num_classes):\n",
        "        super(NeuralDecisionTree, self).__init__()\n",
        "        self.depth = depth\n",
        "        self.num_leaves = 2 ** depth\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Create a mask for the randomly selected features.\n",
        "        num_used_features = int(num_features * used_features_rate)\n",
        "        one_hot = np.eye(num_features)\n",
        "        sampled_feature_indicies = np.random.choice(\n",
        "            np.arange(num_features), num_used_features, replace=False\n",
        "        )\n",
        "        self.used_features_mask = one_hot[sampled_feature_indicies]\n",
        "\n",
        "        # Initialize the weights of the classes in leaves.\n",
        "        self.pi = tf.Variable(\n",
        "            initial_value=tf.random_normal_initializer()(\n",
        "                shape=[self.num_leaves, self.num_classes]\n",
        "            ),\n",
        "            dtype=\"float32\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        # Initialize the stochastic routing layer.\n",
        "        self.decision_fn = layers.Dense(\n",
        "            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n",
        "        )\n",
        "\n",
        "    def call(self, features):\n",
        "        batch_size = tf.shape(features)[0]\n",
        "\n",
        "        # Apply the feature mask to the input features.\n",
        "        features = tf.matmul(\n",
        "            features, self.used_features_mask, transpose_b=True\n",
        "        )  # [batch_size, num_used_features]\n",
        "        # Compute the routing probabilities.\n",
        "        decisions = tf.expand_dims(\n",
        "            self.decision_fn(features), axis=2\n",
        "        )  # [batch_size, num_leaves, 1]\n",
        "        # Concatenate the routing probabilities with their complements.\n",
        "        decisions = layers.concatenate(\n",
        "            [decisions, 1 - decisions], axis=2\n",
        "        )  # [batch_size, num_leaves, 2]\n",
        "\n",
        "        mu = tf.ones([batch_size, 1, 1])\n",
        "\n",
        "        begin_idx = 1\n",
        "        end_idx = 2\n",
        "        # Traverse the tree in breadth-first order.\n",
        "        for level in range(self.depth):\n",
        "            mu = tf.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]\n",
        "            mu = tf.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n",
        "            level_decisions = decisions[\n",
        "                :, begin_idx:end_idx, :\n",
        "            ]  # [batch_size, 2 ** level, 2]\n",
        "            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n",
        "            begin_idx = end_idx\n",
        "            end_idx = begin_idx + 2 ** (level + 1)\n",
        "\n",
        "        mu = tf.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]\n",
        "        probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]\n",
        "        outputs = tf.matmul(mu, probabilities)  # [batch_size, num_classes]\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f2c177a",
      "metadata": {
        "id": "5f2c177a"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "batch_size = 1000\n",
        "num_epochs = 5\n",
        "hidden_units = [128, 64]\n",
        "\n",
        "\n",
        "def run_experiment(model):\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    train_dataset = get_dataset_from_csv(\n",
        "        'train_data_csv.csv', shuffle=False, batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    model.fit(train_dataset, epochs=num_epochs)\n",
        "    print(\"Model training finished\")\n",
        "    \n",
        "    print(\"Evaluating the model on the test data...\")\n",
        "    test_dataset = get_dataset_from_csv2('test_data_csv.csv', batch_size=batch_size)\n",
        "\n",
        "    _, accuracy = model.evaluate(test_dataset)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e1c323f",
      "metadata": {
        "id": "0e1c323f"
      },
      "outputs": [],
      "source": [
        "class NeuralDecisionForest(keras.Model):\n",
        "    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n",
        "        super(NeuralDecisionForest, self).__init__()\n",
        "        self.ensemble = []\n",
        "        # Initialize the ensemble by adding NeuralDecisionTree instances.\n",
        "        # Each tree will have its own randomly selected input features to use.\n",
        "        for _ in range(num_trees):\n",
        "            self.ensemble.append(\n",
        "                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
        "            )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        outputs = tf.zeros([batch_size, num_classes])\n",
        "\n",
        "        # Aggregate the outputs of trees in the ensemble.\n",
        "        for tree in self.ensemble:\n",
        "            outputs += tree(inputs)\n",
        "        # Divide the outputs by the ensemble size to get the average.\n",
        "        outputs /= len(self.ensemble)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "066bc2b1",
      "metadata": {
        "id": "066bc2b1"
      },
      "outputs": [],
      "source": [
        "num_trees = 10\n",
        "depth = 8\n",
        "used_features_rate = 1\n",
        "num_classes = len(TARGET_LABELS)\n",
        "\n",
        "def create_forest_model():\n",
        "    inputs = create_model_inputs()\n",
        "    features = encode_inputs(inputs)\n",
        "    features = layers.BatchNormalization()(features)\n",
        "    num_features = features.shape[1]\n",
        "\n",
        "    forest_model = NeuralDecisionForest(\n",
        "        num_trees, depth, num_features, used_features_rate, num_classes\n",
        "    )\n",
        "\n",
        "    outputs = forest_model(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "forest_model = create_forest_model()\n",
        "\n",
        "fitted_forest = run_experiment(forest_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70b7f6cf",
      "metadata": {
        "id": "70b7f6cf"
      },
      "outputs": [],
      "source": [
        "y_pred = fitted_forest.predict(get_dataset_from_csv(\n",
        "        'train_data_csv.csv', shuffle=False, batch_size=batch_size\n",
        "    ))\n",
        "\n",
        "y_true = df['target']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76d91f12",
      "metadata": {
        "id": "76d91f12"
      },
      "outputs": [],
      "source": [
        "acc = np.dot(1, np.equal(y_true, np.argmax(y_pred, axis=1)))\n",
        "sum(acc)/len(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac6548dc",
      "metadata": {
        "id": "ac6548dc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2558e507",
      "metadata": {
        "id": "2558e507"
      },
      "outputs": [],
      "source": [
        "enc = LabelEncoder()\n",
        "for col in cat_cols[:-1]:\n",
        "    train_data[col] = enc.fit_transform(train_data[col])\n",
        "    test_df[col] = enc.transform(test_df[col])\n",
        "\n",
        "X=train_data.drop(['Target'],axis=1)\n",
        "y=train_data['Target']\n",
        "y_valid, gbm_val_probs, gbm_test_preds, gini=[],[],[],[]\n",
        "ft_importance=pd.DataFrame(index=X.columns)\n",
        "sk_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=21)\n",
        "for fold, (train_idx, val_idx) in enumerate(sk_fold.split(X, y)):\n",
        "    \n",
        "    print(\"\\nFold {}\".format(fold+1))\n",
        "    X_train, y_train = X.iloc[train_idx,:], y[train_idx]\n",
        "    X_val, y_val = X.iloc[val_idx,:], y[val_idx]\n",
        "    print(\"Train shape: {}, {}, Valid shape: {}, {}\\n\".format(\n",
        "        X_train.shape, y_train.shape, X_val.shape, y_val.shape))\n",
        "    \n",
        "    params = {'boosting_type': 'gbdt',\n",
        "              'n_estimators': 1000,\n",
        "              'num_leaves': 50,\n",
        "              'learning_rate': 0.05,\n",
        "              'colsample_bytree': 0.9,\n",
        "              'min_child_samples': 2000,\n",
        "              'max_bins': 500,\n",
        "              'reg_alpha': 2,\n",
        "              'objective': 'binary',\n",
        "              'random_state': 21}\n",
        "    \n",
        "    gbm = LGBMClassifier(**params).fit(X_train, y_train, \n",
        "                                       eval_set=[(X_train, y_train), (X_val, y_val)],\n",
        "                                       callbacks=[early_stopping(200), log_evaluation(500)],\n",
        "                                       eval_metric=['auc','binary_logloss'])\n",
        "    gbm_prob = gbm.predict_proba(X_val)[:,1]\n",
        "    gbm_val_probs.append(gbm_prob)   \n",
        "    \n",
        "    del X_train, y_train, X_val, y_val\n",
        "    _ = gc.collect()\n",
        "    \n",
        "del X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aa83064",
      "metadata": {
        "id": "3aa83064",
        "outputId": "dcee95d8-a4c4-43a5-df25-cdccc2bda841"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'kl' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-1775ba9267e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'kl' is not defined"
          ]
        }
      ],
      "source": [
        "kl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f1a1dd9",
      "metadata": {
        "id": "3f1a1dd9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}